# Syntactic-Abilities-BERT-family

This repo consists of notebook in which I perform experiments to assess the syntactic abilities of the various models belonging to the BERT family viz.
1. bert-base-uncased
2. bert-large-uncased
3. distilbert-base-uncased
4. roberta-base
5. roberta-large

The linguistic phenomena tested in the experiment is that of **subject-verb agreement** involving the following three tasks:
- Simple agreement
- Short VP coordination
- Long VP coordination

The dataset used is the one proposed by Marvin and Linzen in their paper ***Targeted Syntactic Evaluation of Language Models*** and is available at https://github.com/yoavg/bert-syntax

The work done in the notebook is heavily inspired by https://github.com/gsarti/lcl23-xnlm-lab wherein I had the chance to attend the LCL 23 - Lectures on Computational Linguistics held in Pisa. 
